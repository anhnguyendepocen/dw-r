# Scraping Data
Rapid growth of the World Wide Web has significantly changed the way we share, collect, and publish data. Vast amount of information is being stored online, both in structured and unstructured forms.  Regarding certain questions or research topics, this has resulted in a new problem - no longer is the concern of data scarcity and inaccessibility but, rather, one of overcoming the tangled masses of online data. 

Collecting data from the web is not an easy process as there are many technologies used to distribute web content (i.e. [HTML](https://en.wikipedia.org/wiki/HTML), [XML](https://en.wikipedia.org/wiki/XML), [AJAX](https://en.wikipedia.org/wiki/Ajax_(programming)), [JSON](https://en.wikipedia.org/wiki/JSON)). Therefore, dealing with more advanced web scraping requires familiarity with these technologies. Through this chapter I will provide an introduction to some of the fundamental tools required to perform basic web scraping. This includes [importing spreadsheet data files stored online](#importing_spreadsheet_data), [scraping HTML table data](#scraping_HTML_tables), ...

My purpose in the following sections is to discuss these topics at a level meant to get you started in web scraping; however, this area is vast and complex and this chapter will far from provide you expertise level insight.  To advance your knowledge I highly recommend getting copies of [*XML and Web Technologies for Data Sciences with R*](http://www.amazon.com/XML-Web-Technologies-Data-Sciences/dp/1461478995) and [*Automated Data Collection with R*](http://www.amazon.com/Automated-Data-Collection-Practical-Scraping/dp/111883481X/ref=pd_sim_14_1?ie=UTF8&dpID=51Tm7FHxWBL&dpSrc=sims&preST=_AC_UL160_SR108%2C160_&refRID=1VJ1GQEY0VCPZW7VKANX).

## Importing tabular and Excel files stored online {#importing_spreadsheet_data}
The most basic form of getting data from online is to import tabular (i.e. .txt, .csv) or Excel files that are being hosted online. This is often not considered *web scraping*[^fn_scrap1]; however, I think its a good place to start introducing the user to interacting with the web for obtaining data. Importing tabular data is especially common for the many types of government data available online.  A quick perusal of [Data.gov](https://www.data.gov/) illustrates nearly 188,510 examples. In fact, we can provide our first example of importing online tabular data by downloading the Data.gov CSV file that lists all the federal agencies that supply data to Data.gov. 

{linenos=off}
```r
# the url for the online CSV
url <- "https://www.data.gov/media/federal-agency-participation.csv"

# use read.csv to import
data_gov <- read.csv(url, stringsAsFactors = FALSE)

# for brevity I only display first 6 rows
data_gov[1:6,c(1,3:4)]
##                                      Agency.Name Datasets Last.Entry
## 1           Commodity Futures Trading Commission        3 01/12/2014
## 2           Consumer Financial Protection Bureau        2 09/26/2015
## 3           Consumer Financial Protection Bureau        2 09/26/2015
## 4 Corporation for National and Community Service        3 01/12/2014
## 5 Court Services and Offender Supervision Agency        1 01/12/2014
## 6                      Department of Agriculture      698 12/01/2015
```

Downloading Excel spreadsheets hosted online can be performed just as easily.  Recall that there is not a base R function for importing Excel data; however, several packages exist to handle this capability.  One package that works smoothly with pulling Excel data from urls is [`gdata`](https://cran.r-project.org/web/packages/gdata/index.html).  With `gdata` we can use `read.xls()` to download this [Fair Market Rents for Section 8 Housing](http://catalog.data.gov/dataset/fair-market-rents-for-the-section-8-housing-assistance-payments-program) Excel file from the given url. 

{linenos=off}
```r
library(gdata)

# the url for the online Excel file
url <- "http://www.huduser.org/portal/datasets/fmr/fmr2015f/FY2015F_4050_Final.xls"

# use read.xls to import
rents <- read.xls(url)

rents[1:6, 1:10]
##    fips2000  fips2010 fmr2 fmr0 fmr1 fmr3 fmr4 county State CouSub
## 1 100199999 100199999  788  628  663 1084 1288      1     1  99999
## 2 100399999 100399999  762  494  643 1123 1318      3     1  99999
## 3 100599999 100599999  670  492  495  834  895      5     1  99999
## 4 100799999 100799999  773  545  652 1015 1142      7     1  99999
## 5 100999999 100999999  773  545  652 1015 1142      9     1  99999
## 6 101199999 101199999  599  481  505  791 1061     11     1  99999
```

Note that many of the arguments covered in the [Importing Data chapter](#excel) (i.e. specifying sheets to read from, skipping lines) also apply to `read.xls()`. In addition, `gdata` provides some useful functions (`sheetCount()` and `sheetNames()`) for identifying if multiple sheets exist prior to downloading.

Another common form of file storage is using zip files.  For instance, the [Bureau of Labor Statistics](http://www.bls.gov/home.htm) (BLS) stores their [public-use microdata](http://www.bls.gov/cex/pumdhome.htm) for the [Consumer Expenditure Survey](http://www.bls.gov/cex/home.htm) in .zip files.  We can use `download.file()` to download the file to your working directory and then work with this data as desired.

{linenos=off}
```r
url <- "http://www.bls.gov/cex/pumd/data/comma/diary14.zip"

# download .zip file and unzip contents
download.file(url, dest="dataset.zip", mode="wb") 
unzip ("dataset.zip", exdir = "./")

# assess the files contained in the .zip file which
# unzips as a folder named "diary14"
list.files("diary14")
##  [1] "dtbd141.csv" "dtbd142.csv" "dtbd143.csv" "dtbd144.csv" "dtid141.csv"
##  [6] "dtid142.csv" "dtid143.csv" "dtid144.csv" "expd141.csv" "expd142.csv"
## [11] "expd143.csv" "expd144.csv" "fmld141.csv" "fmld142.csv" "fmld143.csv"
## [16] "fmld144.csv" "memd141.csv" "memd142.csv" "memd143.csv" "memd144.csv"

# alternatively, if we know the file we want prior to unzipping
# we can extract the file without unzipping using unz():
zip_data <- read.csv(unz("dataset.zip", "diary14/expd141.csv"))
zip_data[1:5, 1:10]
##     NEWID ALLOC COST GIFT PUB_FLAG    UCC EXPNSQDY EXPN_QDY EXPNWKDY   EXPN_KDY
## 1 2825371     0 6.26    2        2 190112        1        D        3        D
## 2 2825371     0 1.20    2        2 190322        1        D        3        D
## 3 2825381     0 0.98    2        2  20510        3        D        2        D
## 4 2825381     0 0.98    2        2  20510        3        D        2        D
## 5 2825381     0 2.50    2        2  20510        3        D        2        D

```

The .zip archive file format is meant to compress files and are typically used on files of significant size.  For instance, the Consumer Expenditure Survey data we downloaded in the previous example is over 10MB.  Obviously there may be times in which we want to get specific data in the .zip file to analyze but not always permanently store the entire .zip file contents. In these instances we can use the following [process](http://stackoverflow.com/questions/3053833/using-r-to-download-zipped-data-file-extract-and-import-data) proposed by [Dirk Eddelbuettel](https://twitter.com/eddelbuettel) to temporarily download the .zip file, extract the desired data, and then discard the .zip file.

{linenos=off}
```r
# Create a temp. file name
temp <- tempfile()

# Use download.file() to fetch the file into the temp. file
download.file("http://www.bls.gov/cex/pumd/data/comma/diary14.zip",temp)

# Use unz() to extract the target file from temp. file
zip_data2 <- read.csv(unz(temp, "diary14/expd141.csv"))

# Remove the temp file via unlink()
unlink(temp)

zip_data2[1:5, 1:10]
##     NEWID ALLOC COST GIFT PUB_FLAG    UCC EXPNSQDY EXPN_QDY EXPNWKDY   EXPN_KDY
## 1 2825371     0 6.26    2        2 190112        1        D        3        D
## 2 2825371     0 1.20    2        2 190322        1        D        3        D
## 3 2825381     0 0.98    2        2  20510        3        D        2        D
## 4 2825381     0 0.98    2        2  20510        3        D        2        D
## 5 2825381     0 2.50    2        2  20510        3        D        2        D
```

One last common scenario I'll cover when importing spreadsheet data from online is when we identify multiple data sets that we'd like to download but are not centrally stored in a .zip format or the like. As a simple example lets look at the [average consumer price data](http://www.bls.gov/data/#prices) from the BLS. The BLS holds multiple data sets for different types of commodities within one [url](http://download.bls.gov/pub/time.series/ap/); however, there are separate links for each individual data set.  More complicated cases of this will have the links to tabular data sets scattered throughout a webpage[^fn_scrap2]. The [`XML`](https://cran.r-project.org/web/packages/XML/index.html) package provides the useful `getHTMLLinks()` function to identify these links.

{linenos=off}
```r
library(XML)

# url hosting multiple links to data sets
url <- "http://download.bls.gov/pub/time.series/ap/"

# identify the links available
links <- getHTMLLinks(url)

links
##  [1] "/pub/time.series/"                           
##  [2] "/pub/time.series/ap/ap.area"                 
##  [3] "/pub/time.series/ap/ap.contacts"             
##  [4] "/pub/time.series/ap/ap.data.0.Current"       
##  [5] "/pub/time.series/ap/ap.data.1.HouseholdFuels"
##  [6] "/pub/time.series/ap/ap.data.2.Gasoline"      
##  [7] "/pub/time.series/ap/ap.data.3.Food"          
##  [8] "/pub/time.series/ap/ap.footnote"             
##  [9] "/pub/time.series/ap/ap.item"                 
## [10] "/pub/time.series/ap/ap.period"               
## [11] "/pub/time.series/ap/ap.series"               
## [12] "/pub/time.series/ap/ap.txt"
```

This allows us to assess which files exist that may be of interest.  In this case the links that we are primarily interested in are the ones that contain "data" in their name (links 4-7 listed above).  We can use the [`stringr`](https://cran.r-project.org/web/packages/stringr/index.html) package to extract these desired links which we will use to download the data.

{linenos=off}
```r
library(stringr)

# extract names for desired links and paste to url
links_data <- links[str_detect(links, "data")]

# paste url to data links to have full url for data sets
# use str_sub and regexpr to paste links at appropriate 
# starting point
filenames <- paste0(url, str_sub(links_data, start = regexpr("ap.data", links_data)))

filenames
## [1] "http://download.bls.gov/pub/time.series/ap/ap.data.0.Current"       
## [2] "http://download.bls.gov/pub/time.series/ap/ap.data.1.HouseholdFuels"
## [3] "http://download.bls.gov/pub/time.series/ap/ap.data.2.Gasoline"      
## [4] "http://download.bls.gov/pub/time.series/ap/ap.data.3.Food"
```

We can now proceed to develop a simple `for` loop function to download each data set. We store the results in a list which contains 4 items, one item for each data set.  Each list item contains the url in which the data was extracted from and the dataframe containing the downloaded data.  We're now ready to analyze these data sets as necessary. 

{linenos=off}
```r
# create empty list to dump data into
data_ls <- list()

for(i in 1:length(filenames)){
        url <- filenames[i]
        data <- read.delim(url)
        data_ls[[length(data_ls) + 1]] <- list(url=filenames[i], data=data)
}

str(data_ls)
## List of 4
##  $ :List of 2
##   ..$ url : chr "http://download.bls.gov/pub/time.series/ap/ap.data.0.Current"
##   ..$ data:'data.frame':	144712 obs. of  5 variables:
##   .. ..$ series_id     : Factor w/ 878 levels "APU0000701111    ",..: 1 1 1 1 1 1 1 1 1 1 ...
##   .. ..$ year          : int [1:144712] 1995 1995 1995 1995 1995 1995 1995 1995 1995 1995 ...
##   .. ..$ period        : Factor w/ 12 levels "M01","M02","M03",..: 1 2 3 4 5 6 7 8 9 10 ...
##   .. ..$ value         : num [1:144712] 0.238 0.242 0.242 0.236 0.244 0.244 0.248 0.255 0.256 0.254 ...
##   .. ..$ footnote_codes: logi [1:144712] NA NA NA NA NA NA ...
##  $ :List of 2
##   ..$ url : chr "http://download.bls.gov/pub/time.series/ap/ap.data.1.HouseholdFuels"
##   ..$ data:'data.frame':	90339 obs. of  5 variables:
##   .. ..$ series_id     : Factor w/ 343 levels "APU000072511     ",..: 1 1 1 1 1 1 1 1 1 1 ...
##   .. ..$ year          : int [1:90339] 1978 1978 1979 1979 1979 1979 1979 1979 1979 1979 ...
##   .. ..$ period        : Factor w/ 12 levels "M01","M02","M03",..: 11 12 1 2 3 4 5 6 7 8 ...
##   .. ..$ value         : num [1:90339] 0.533 0.545 0.555 0.577 0.605 0.627 0.656 0.709 0.752 0.8 ...
##   .. ..$ footnote_codes: logi [1:90339] NA NA NA NA NA NA ...
##  $ :List of 2
##   ..$ url : chr "http://download.bls.gov/pub/time.series/ap/ap.data.2.Gasoline"
##   ..$ data:'data.frame':	69357 obs. of  5 variables:
##   .. ..$ series_id     : Factor w/ 341 levels "APU000074712     ",..: 1 1 1 1 1 1 1 1 1 1 ...
##   .. ..$ year          : int [1:69357] 1973 1973 1973 1974 1974 1974 1974 1974 1974 1974 ...
##   .. ..$ period        : Factor w/ 12 levels "M01","M02","M03",..: 10 11 12 1 2 3 4 5 6 7 ...
##   .. ..$ value         : num [1:69357] 0.402 0.418 0.437 0.465 0.491 0.528 0.537 0.55 0.556 0.558 ...
##   .. ..$ footnote_codes: logi [1:69357] NA NA NA NA NA NA ...
##  $ :List of 2
##   ..$ url : chr "http://download.bls.gov/pub/time.series/ap/ap.data.3.Food"
##   ..$ data:'data.frame':	122302 obs. of  5 variables:
##   .. ..$ series_id     : Factor w/ 648 levels "APU0000701111    ",..: 1 1 1 1 1 1 1 1 1 1 ...
##   .. ..$ year          : int [1:122302] 1980 1980 1980 1980 1980 1980 1980 1980 1980 1980 ...
##   .. ..$ period        : Factor w/ 12 levels "M01","M02","M03",..: 1 2 3 4 5 6 7 8 9 10 ...
##   .. ..$ value         : num [1:122302] 0.203 0.205 0.211 0.206 0.207 0.21 0.214 0.215 0.214 0.212 ...
##   .. ..$ footnote_codes: logi [1:122302] NA NA NA NA NA NA ...
```

These examples provide the basics required for downloading most tabular and Excel files from online. However, this is just the beginning of importing/scraping data from the web.  Next, we'll start exploring the more conventional forms scraping text and data stored on HTML webpages.

## Scraping HTML table data {#scraping_HTML_tables}

The next common structure of data storage on the Web is in the form of HTML tables. The simplest approach to scraping HTML table data directly into R is by using either the [XML package](#scraping_tables_xml) or the [`rvest` package](#scraping_tables_rvest).  I will illustrate with this [BLS employment statistics webpage](http://www.bls.gov/web/empsit/cesbmart.htm) which contains multiple HTML tables.  

### Scraping HTML tables with XML {#scraping_tables_xml}
The XML package provides a convenient `readHTMLTable()` function to extract data from HTML tables in HTML documents.  By passing the URL to `readHTMLTable()`, the data in each table is read and stored as a data frame.  In a case like this where multiple tables exists, the data frames will be stored in a list as illustrated.

{linenos=off}
```r
library(XML)

url <- "http://www.bls.gov/web/empsit/cesbmart.htm"

# read in HTML data
tbls <- readHTMLTable(url)

typeof(tbls)
## [1] "list"

length(tbls)
## [1] 15
```

The list `tbl` contains 15 items.  This includes data from the 10 data tables seen on the webpage but also includes data from a few additional tables used to format parts of the page (i.e. table of contents, table of figures, advertisements). Using `str(tbl)`, you can investigate the data frames in the list to help identify the data you are interested in pulling.  Lets assume we are interested in pulling data from the second and third tables on the webpage (i) *Table 2. Nonfarm employment benchmarks by industry, March 2014 (in thousands)* and (ii) *Table 3. Net birth/death estimates by industry supersector, April – December 2014 (in thousands)*.  By assessing `str(tbl)` you will see that these data are captured as the third and fourth items in `tbl` with the convenient titles `Table2` and `Table3` respectively. 

You can access these two data frames via normal list [subsetting](#lists_subset) or by using the `which` argument in `readHTMLTable()` which restricts the data importing to only those tables specified.  Also, note that the variables in Table 2 have variable names: `V1`, `V2`,...,`V8`.  When HTML tables have split headers as is the case with Table 2, the variable names are stripped and replaced with generic names because R does not know which variable names should align with each column.

{linenos=off}
```r
head(tbls[[3]])
##          V1                        V2      V3      V4  V5   V6
## 1 00-000000             Total nonfarm 137,214 137,147  67  (1)
## 2 05-000000             Total private 114,989 114,884 105  0.1
## 3 06-000000           Goods-producing  18,675  18,558 117  0.6
## 4 07-000000         Service-providing 118,539 118,589 -50  (1)
## 5 08-000000 Private service-providing  96,314  96,326 -12  (1)
## 6 10-000000        Mining and logging     868     884 -16 -1.8

head(tbls[[4]], 3)
##   CES Industry Code CES Industry Title Apr May Jun Jul Aug Sep Oct Nov Dec
## 1         10-000000 Mining and logging   2   2   2   2   1   1   1   1   0
## 2         20-000000       Construction  35  37  24  12  12   7  12 -10 -21
## 3         30-000000      Manufacturing   0   6   4  -3   4   1   3   2   0
##   CumulativeTotal
## 1              12
## 2             108
## 3              17

# an alternative is to explicitly state which 
# table(s) to download
emp_ls <- readHTMLTable(url, which = c(3,4))

str(emp_ls)
## List of 2
##  $ Table2:'data.frame':	145 obs. of  6 variables:
##   ..$ V1: Factor w/ 145 levels "00-000000","05-000000",..: 1 2 3 4 5 6 7 8 9 10 ...
##   ..$ V2: Factor w/ 143 levels "Accommodation",..: 130 131 52 116 102 74 67 73 90 75 ...
##   ..$ V3: Factor w/ 145 levels "1,010.3","1,048.3",..: 40 35 48 37 145 140 109 135 51 65 ...
##   ..$ V4: Factor w/ 145 levels "1,008.4","1,052.3",..: 41 34 48 36 144 142 109 136 66 65 ...
##   ..$ V5: Factor w/ 123 levels "-0.3","-0.4",..: 113 68 71 48 9 19 29 11 12 43 ...
##   ..$ V6: Factor w/ 56 levels "-0.1","-0.2",..: 30 31 36 30 30 16 28 14 29 22 ...
##  $ Table3:'data.frame':	11 obs. of  12 variables:
##   ..$ CES Industry Code : Factor w/ 11 levels "10-000000","20-000000",..: 1 2 3 4 5 6 7 8 9 10 ...
##   ..$ CES Industry Title: Factor w/ 11 levels "263","Construction",..: 8 2 7 11 5 4 10 3 6 9 ...
##   ..$ Apr               : Factor w/ 10 levels "0","12","2","204",..: 3 7 1 5 1 8 9 6 10 2 ...
##   ..$ May               : Factor w/ 10 levels "129","13","2",..: 3 6 8 5 7 9 4 2 10 8 ...
##   ..$ Jun               : Factor w/ 10 levels "-14","0","12",..: 5 6 7 3 2 7 8 1 10 9 ...
##   ..$ Jul               : Factor w/ 10 levels "-1","-2","-3",..: 6 5 3 10 1 7 8 10 9 2 ...
##   ..$ Aug               : Factor w/ 9 levels "-19","1","12",..: 2 3 9 4 8 9 5 6 7 8 ...
##   ..$ Sep               : Factor w/ 9 levels "-1","-12","-2",..: 5 8 5 9 1 1 2 6 4 3 ...
##   ..$ Oct               : Factor w/ 10 levels "-17","1","12",..: 2 3 6 5 9 4 10 7 1 8 ...
##   ..$ Nov               : Factor w/ 8 levels "-10","-15","-22",..: 4 1 7 5 8 8 6 6 3 4 ...
##   ..$ Dec               : Factor w/ 8 levels "-10","-21","-3",..: 4 2 4 7 4 6 1 3 7 5 ...
##   ..$ CumulativeTotal   : Factor w/ 10 levels "107","108","12",..: 3 2 6 4 5 10 7 1 8 9 ...
```

### Scraping HTML tables with rvest {#scraping_tables_rvest}

[`rvest`](https://cran.r-project.org/web/packages/rvest/index.html) is a relatively newer package created by the RStudio team inspired by libraries such as [beautiful soup](http://www.crummy.com/software/BeautifulSoup/). `rvest` provides multiple functionalities such as extracting tag names, text, data, and attributes; detecting and repairing encoding issues; navigating around a website and more.  However, this section focuses only on extracting HTML table data with `rvest`.

To extract the table(s) of interest we first use the `html_nodes()` function to select the CSS nodes of interest.  In this case we are interested in all table nodes that exist in the webpage. `html_nodes` will capture all 15 HTML tables similar to `XML`'s `readHTMLTable()`. However, `html_nodes` does not parse the data; rather, it acts as a CSS selector.

{linenos=off}
```r
library(rvest)

webpage <- read_html("http://www.bls.gov/web/empsit/cesbmart.htm")

tbls <- html_nodes(webpage, "table")

head(tbls)
## {xml_nodeset (6)}
## [1] <table id="main-content-table">&#13;\n\t<tr>&#13;\n\t\t<td id="secon ...
## [2] <table id="Table1" class="regular" cellspacing="0" cellpadding="0" x ...
## [3] <table id="Table2" class="regular" cellspacing="0" cellpadding="0" x ...
## [4] <table id="Table3" class="regular" cellspacing="0" cellpadding="0" x ...
## [5] <table id="Table4" class="regular" cellspacing="0" cellpadding="0" x ...
## [6] <table id="Exhibit1" class="regular" cellspacing="0" cellpadding="0" ...
```

Looking through `tbls` we can see that the two tables of interest ("Table2" & "Table3") are captured as list items 3 & 4 above. We can now use `html_table()` to parse the HTML tables into data frames.  Since we are extracting two tables `html_table()` will create a list containing two data frames for the HTML tables parsed. Also, note that `rvest` makes use of the pipe operator (`%>%`) developed through the `magrittr` package.  The functionality of this operator is discussed in more detail in a later [chapter covering the %>% operator](#pipe).

{linenos=off}
```r
webpage <- read_html("http://www.bls.gov/web/empsit/cesbmart.htm")

tbls_ls <- webpage %>%
        html_nodes("table") %>%
        .[3:4] %>%
        html_table(fill = TRUE)

str(tbls_ls)
## List of 2
##  $ :'data.frame':	147 obs. of  6 variables:
##   ..$ CES Industry Code : chr [1:147] "Amount" "00-000000" "05-000000" "06-000000" ...
##   ..$ CES Industry Title: chr [1:147] "Percent" "Total nonfarm" "Total private" "Goods-producing" ...
##   ..$ Benchmark         : chr [1:147] NA "137,214" "114,989" "18,675" ...
##   ..$ Estimate          : chr [1:147] NA "137,147" "114,884" "18,558" ...
##   ..$ Differences       : num [1:147] NA 67 105 117 -50 -12 -16 -2.8 -13.2 -13.5 ...
##   ..$ NA                : chr [1:147] NA "(1)" "0.1" "0.6" ...
##  $ :'data.frame':	11 obs. of  12 variables:
##   ..$ CES Industry Code : chr [1:11] "10-000000" "20-000000" "30-000000" "40-000000" ...
##   ..$ CES Industry Title: chr [1:11] "Mining and logging" "Construction" "Manufacturing" "Trade, transportation, and utilities" ...
##   ..$ Apr               : int [1:11] 2 35 0 21 0 8 81 22 82 12 ...
##   ..$ May               : int [1:11] 2 37 6 24 5 8 22 13 81 6 ...
##   ..$ Jun               : int [1:11] 2 24 4 12 0 4 5 -14 86 6 ...
##   ..$ Jul               : int [1:11] 2 12 -3 7 -1 3 35 7 62 -2 ...
##   ..$ Aug               : int [1:11] 1 12 4 14 3 4 19 21 23 3 ...
##   ..$ Sep               : int [1:11] 1 7 1 9 -1 -1 -12 12 -33 -2 ...
##   ..$ Oct               : int [1:11] 1 12 3 28 6 16 76 35 -17 4 ...
##   ..$ Nov               : int [1:11] 1 -10 2 10 3 3 14 14 -22 1 ...
##   ..$ Dec               : int [1:11] 0 -21 0 4 0 10 -10 -3 4 1 ...
##   ..$ CumulativeTotal   : int [1:11] 12 108 17 129 15 55 230 107 266 29 ...
```

One difference to note when using `rvest`'s `html_table` versus using `XML`'s `readHTMLTable` is when reading split column headings.  As we saw earlier in this chapter, `readHTMLTable` will cause split headings to be stripped and replaced with generic "VX" titles.  `html_table` , on other hand, will cause split headings to be included and can cause the first row to include parts of the headings.  We can see this with Table 2.  This may require some data clean up.

{linenos=off}
```r
head(tbls_ls[[1]], 4)
##   CES Industry Code CES Industry Title Benchmark Estimate Differences   NA
## 1            Amount            Percent      <NA>     <NA>          NA <NA>
## 2         00-000000      Total nonfarm   137,214  137,147          67  (1)
## 3         05-000000      Total private   114,989  114,884         105  0.1
## 4         06-000000    Goods-producing    18,675   18,558         117  0.6

# remove row 1 that includes part of the headings
tbls_ls[[1]] <- tbls_ls[[1]][-1,]

# rename table headings
colnames(tbls_ls[[1]]) <- c("CES_Code", "Ind_Title", "Benchmark",
                            "Estimate", "Amt_Diff", "Pct_Diff")

head(tbls_ls[[1]], 4)
##    CES_Code         Ind_Title Benchmark Estimate Amt_Diff Pct_Diff
## 2 00-000000     Total nonfarm   137,214  137,147       67      (1)
## 3 05-000000     Total private   114,989  114,884      105      0.1
## 4 06-000000   Goods-producing    18,675   18,558      117      0.6
## 5 07-000000 Service-providing   118,539  118,589      -50      (1)
```

This section gives you a taste for how simple scraping data from HTML tables can be.  Next we'll take a look at how you can scrape...

## Scraping HTML text {#scraping_HTML_text}


## Scraping data via APIs



[^fn_scrap1]: In [Automated Data Collection with R](http://www.amazon.com/Automated-Data-Collection-Practical-Scraping/dp/111883481X/ref=pd_sim_14_1?ie=UTF8&dpID=51Tm7FHxWBL&dpSrc=sims&preST=_AC_UL160_SR108%2C160_&refRID=1VJ1GQEY0VCPZW7VKANX") Munzert et al. state that "[t]he first way to get data from the web is almost too banal to be considered here and actually not a case of web scraping in the narrower sense."

[^fn_scrap2]: An example is provided in [Automated Data Collection with R](http://www.amazon.com/Automated-Data-Collection-Practical-Scraping/dp/111883481X/ref=pd_sim_14_1?ie=UTF8&dpID=51Tm7FHxWBL&dpSrc=sims&preST=_AC_UL160_SR108%2C160_&refRID=1VJ1GQEY0VCPZW7VKANX") in which they use a similar approach to extract desired CSV files scattered throughout the [Maryland State Board of Elections websiteMaryland State Board of Elections website](http://www.elections.state.md.us/elections/2012/election_data/index.html).


